# -*- coding: utf-8 -*-
"""simple_linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mx8giBYn0N545coqyH_CZFB89WFloZ3G

<h2 align="center">Simple Linear Regression</h2>

Linear Regression is a useful tool for predicting a quantitative response.

We have an input vector $X^T = (X_1, X_2,...,X_p)$, and want to predict a real-valued output $Y$. The linear regression model has the form

<h4 align="center"> $f(x) = \beta_0 + \sum_{j=1}^p X_j \beta_j$. </h4>

The linear model either assumes that the regression function $E(Y|X)$ is linear, or that the linear model is a reasonable approximation.Here the $\beta_j$'s are unknown parameters or coefficients, and the variables $X_j$ can come from different sources. No matter the source of $X_j$, the model is linear in the parameters.

### Loading the Data and Importing Libraries
---
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

"""The adverstiting dataset captures sales revenue generated with respect to advertisement spends across multiple channles like radio, tv and newspaper. [Source](http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv)"""

data = pd.read_csv('Advertising.csv')

data.head()

data.info()

data.isnull().sum()

"""### Remove the index column"""

data = data.drop(['Unnamed: 0'],axis =1)

data.shape

data.head()

"""### Exploratory Data Analysis"""

sns.distplot(data.sales,kde=False)

sns.distplot(data.TV,kde=False)

sns.distplot(data.radio,kde=False)

sns.distplot(data.newspaper,kde=False)

"""### Exploring Relationships between Predictors and Response"""

sns.pairplot(data=data,x_vars=['TV','radio','newspaper'],y_vars='sales',height = 8,aspect=1,kind='reg')

data.corr()

np.round(data.corr(),2)

plt.figure(dpi=100)
sns.heatmap(np.round(data.corr(),2),annot =True)

"""### Creating the Simple Linear Regression Model

General linear regression model:
$y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+...+\beta_{n}x_{n}$

- $y$  is the response
- $\beta_{0}$ is the intercept
- $\beta_{1}$ is the coefficient for  x1  (the first feature)
- $\beta_{n}$ is the coefficient for  xn  (the nth feature)

In our case: $y=\beta_{0}+\beta_{1}×TV+\beta_{2}×Radio+\beta_{3}×Newspaper$

The $\beta$ values are called the **model coefficients*:

- These values are "learned" during the model fitting step using the "least squares" criterion
- The fitted model is then used to make predictions
"""

X= data[['TV']]
X.head()

y = data[['sales']]
y.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test = train_test_split(X,y,random_state=1)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

from sklearn.linear_model import LinearRegression
linreg = LinearRegression()
linreg.fit(X_train,y_train)

"""### Interpreting Model Coefficients"""

print(linreg.intercept_)
print(linreg.coef_)

"""### Making Predictions with our Model"""

y_pred = linreg.predict(X_test)
y_pred[:5]

"""### Model Evaluation Metrics

**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:;
$$ \frac{1}{n} \sum_{i=1}^{n} \left |y_i - \hat{y}_i \right |$$

**Mean Squared Error** (MSE) is the mean of the squared errors:
$$\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:
$$\sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$
"""

print('Training Score')
print(linreg.score(X_train,y_train))

print('Test Score')
print(linreg.score(X_test,y_test))

from sklearn import metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
mean_squ_error = mean_squared_error(y_test, y_pred)
mean_abs_error = mean_absolute_error(y_test, y_pred)
print("Mean Squared Error:",mean_squ_error)
print("Mean absolute Error:",mean_abs_error)